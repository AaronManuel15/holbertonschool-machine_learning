{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import GPyOpt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m optimizer \u001b[39m=\u001b[39m GPyOpt\u001b[39m.\u001b[39mmethods\u001b[39m.\u001b[39mBayesianOptimization(f\u001b[39m=\u001b[39mobjective_function, domain\u001b[39m=\u001b[39mbounds)\n\u001b[1;32m     46\u001b[0m \u001b[39m# Run the optimization\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m optimizer\u001b[39m.\u001b[39;49mrun_optimization(max_iter\u001b[39m=\u001b[39;49mmax_iterations, report_file\u001b[39m=\u001b[39;49mreport_file, verbosity\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     49\u001b[0m \u001b[39m# Get the best hyperparameters and objective value\u001b[39;00m\n\u001b[1;32m     50\u001b[0m best_hyperparams \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mx_opt\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/GPyOpt/core/bo.py:137\u001b[0m, in \u001b[0;36mBO.run_optimization\u001b[0;34m(self, max_iter, max_time, eps, context, verbosity, save_models_parameters, report_file, evaluations_file, models_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_time \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcum_time):\n\u001b[1;32m    135\u001b[0m     \u001b[39m# --- Update model\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalization_type)\n\u001b[1;32m    138\u001b[0m     \u001b[39mexcept\u001b[39;00m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mLinAlgError:\n\u001b[1;32m    139\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/GPyOpt/core/bo.py:253\u001b[0m, in \u001b[0;36mBO._update_model\u001b[0;34m(self, normalization_type)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         Y_inmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mY\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mupdateModel(X_inmodel, Y_inmodel, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    255\u001b[0m \u001b[39m# Save parameters of the model\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_model_parameter_values()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/GPyOpt/models/gpmodel.py:93\u001b[0m, in \u001b[0;36mGPModel.updateModel\u001b[0;34m(self, X_all, Y_all, X_new, Y_new)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39moptimize(optimizer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, max_iters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iters, messages\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, ipython_notebook\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     92\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49moptimize_restarts(num_restarts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimize_restarts, optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, max_iters \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iters, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/paramz/model.py:182\u001b[0m, in \u001b[0;36mModel.optimize_restarts\u001b[0;34m(self, num_restarts, robust, verbose, parallel, num_processes, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[39mprint\u001b[39m((\u001b[39m\"\u001b[39m\u001b[39mWarning - optimization restart \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m failed\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, num_restarts)))\n\u001b[1;32m    181\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimization_runs) \u001b[39m>\u001b[39m initial_length:\n\u001b[1;32m    185\u001b[0m     \u001b[39m# This works, since failed jobs don't get added to the optimization_runs.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmin([o\u001b[39m.\u001b[39mf_opt \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimization_runs[initial_length:]])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/paramz/model.py:171\u001b[0m, in \u001b[0;36mModel.optimize_restarts\u001b[0;34m(self, num_restarts, robust, verbose, parallel, num_processes, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parallel:\n\u001b[1;32m    170\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandomize()\n\u001b[1;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\u001b[39m#pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/GPy/core/__init__.py:80\u001b[0m, in \u001b[0;36mrandomize\u001b[0;34m(self, rand_gen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_array\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     79\u001b[0m [np\u001b[39m.\u001b[39mput(x, ind, p\u001b[39m.\u001b[39mrvs(ind\u001b[39m.\u001b[39msize)) \u001b[39mfor\u001b[39;00m p, ind \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpriors\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m p \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m---> 80\u001b[0m unfixlist \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize,),dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mbool)\n\u001b[1;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mparamz\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransformations\u001b[39;00m \u001b[39mimport\u001b[39;00m __fixed__\n\u001b[1;32m     82\u001b[0m unfixlist[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstraints[__fixed__]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/__init__.py:284\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m Tester\n\u001b[1;32m    282\u001b[0m     \u001b[39mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 284\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'"
     ]
    }
   ],
   "source": [
    "# Define the objective function to optimize\n",
    "def objective_function(parameters):\n",
    "    learning_rate = parameters[0][0]\n",
    "    hidden_units = int(parameters[0][1])\n",
    "    dropout_rate = parameters[0][2]\n",
    "    l2_reg_weight = parameters[0][3]\n",
    "    batch_size = int(parameters[0][4])\n",
    "\n",
    "    # Create and train the model\n",
    "    model = MLPClassifier(hidden_layer_sizes=(hidden_units,),\n",
    "                          learning_rate_init=learning_rate,\n",
    "                          alpha=l2_reg_weight,\n",
    "                          batch_size=batch_size,\n",
    "                          early_stopping=True,\n",
    "                          validation_fraction=0.1,\n",
    "                          n_iter_no_change=5,\n",
    "                          random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return -accuracy  # Negative sign for maximization\n",
    "\n",
    "# Define the bounds and types of the hyperparameters to optimize\n",
    "bounds = [{'name': 'learning_rate', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "          {'name': 'hidden_units', 'type': 'discrete', 'domain': (50, 100, 200)},\n",
    "          {'name': 'dropout_rate', 'type': 'continuous', 'domain': (0.0, 0.5)},\n",
    "          {'name': 'l2_reg_weight', 'type': 'continuous', 'domain': (0.0001, 0.01)},\n",
    "          {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 32, 64)}]\n",
    "\n",
    "# Set the number of optimization iterations\n",
    "max_iterations = 30\n",
    "\n",
    "# Set the output file for the optimization report\n",
    "report_file = 'bayes_opt.txt'\n",
    "\n",
    "# Set the checkpoint filename format\n",
    "checkpoint_format = 'checkpoint_lr_{}_hu_{}_dr_{}_l2_{}_bs_{}.h5'\n",
    "\n",
    "# Initialize the Bayesian optimizer\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=objective_function, domain=bounds)\n",
    "\n",
    "# Run the optimization\n",
    "optimizer.run_optimization(max_iter=max_iterations, report_file=report_file, verbosity=True)\n",
    "\n",
    "# Get the best hyperparameters and objective value\n",
    "best_hyperparams = optimizer.x_opt\n",
    "best_objective = optimizer.fx_opt\n",
    "\n",
    "# Print the best hyperparameters and objective value\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Learning Rate:\", best_hyperparams[0][0])\n",
    "print(\"Hidden Units:\", int(best_hyperparams[0][1]))\n",
    "print(\"Dropout Rate:\", best_hyperparams[0][2])\n",
    "print(\"L2 Regularization Weight:\", best_hyperparams[0][3])\n",
    "print(\"Batch Size:\", int(best_hyperparams[0][4]))\n",
    "print(\"Best Objective Value:\", -best_objective)\n",
    "\n",
    "# Plot the convergence\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(optimizer.Y_best[1:])\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Best Objective Value')\n",
    "plt.title('Convergence Plot')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'alpha' parameter of MLPClassifier must be a float in the range [0, inf). Got array([0.00321495]) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m checkpoint_format \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcheckpoint_lr_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_hu_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_dr_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_l2_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_bs_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[39m# Initialize the Bayesian optimizer\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m optimizer \u001b[39m=\u001b[39m GPyOpt\u001b[39m.\u001b[39;49mmethods\u001b[39m.\u001b[39;49mBayesianOptimization(f\u001b[39m=\u001b[39;49mobjective_function, domain\u001b[39m=\u001b[39;49mbounds)\n\u001b[1;32m     54\u001b[0m \u001b[39m# Run the optimization\u001b[39;00m\n\u001b[1;32m     55\u001b[0m optimizer\u001b[39m.\u001b[39mrun_optimization(max_iter\u001b[39m=\u001b[39mmax_iterations, report_file\u001b[39m=\u001b[39mreport_file, verbosity\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/GPyOpt/methods/bayesian_optimization.py:119\u001b[0m, in \u001b[0;36mBayesianOptimization.__init__\u001b[0;34m(self, f, domain, constraints, cost_withGradients, model_type, X, Y, initial_design_numdata, initial_design_type, acquisition_type, normalize_Y, exact_feval, acquisition_optimizer_type, model_update_interval, evaluator_type, batch_size, num_cores, verbosity, verbosity_model, maximize, de_duplication, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_design_type  \u001b[39m=\u001b[39m initial_design_type\n\u001b[1;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_design_numdata \u001b[39m=\u001b[39m initial_design_numdata\n\u001b[0;32m--> 119\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_design_chooser()\n\u001b[1;32m    121\u001b[0m \u001b[39m# --- CHOOSE the model type. If an instance of a GPyOpt model is passed (possibly user defined), it is used.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m=\u001b[39m model_type\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/GPyOpt/methods/bayesian_optimization.py:195\u001b[0m, in \u001b[0;36mBayesianOptimization._init_design_chooser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX \u001b[39m=\u001b[39m initial_design(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_design_type, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspace, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial_design_numdata)\n\u001b[0;32m--> 195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mY, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjective\u001b[39m.\u001b[39;49mevaluate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX)\n\u001b[1;32m    196\u001b[0m \u001b[39m# Case 2\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mY \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/GPyOpt/core/task/objective.py:50\u001b[0m, in \u001b[0;36mSingleObjective.evaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mPerforms the evaluation of the objective at x.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_procs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 50\u001b[0m     f_evals, cost_evals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_eval_func(x)\n\u001b[1;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/GPyOpt/core/task/objective.py:74\u001b[0m, in \u001b[0;36mSingleObjective._eval_func\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     73\u001b[0m     st_time    \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 74\u001b[0m     rlt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(np\u001b[39m.\u001b[39;49matleast_2d(x[i]))\n\u001b[1;32m     75\u001b[0m     f_evals     \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([f_evals,rlt])\n\u001b[1;32m     76\u001b[0m     cost_evals \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mst_time]\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mobjective_function\u001b[0;34m(parameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Create and train the model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m model \u001b[39m=\u001b[39m MLPClassifier(hidden_layer_sizes\u001b[39m=\u001b[39mhidden_units,\n\u001b[1;32m     19\u001b[0m                       learning_rate_init\u001b[39m=\u001b[39mlearning_rate,\n\u001b[1;32m     20\u001b[0m                       alpha\u001b[39m=\u001b[39ml2_reg_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m                       n_iter_no_change\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m     25\u001b[0m                       random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     29\u001b[0m \u001b[39m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     30\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:747\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[1;32m    731\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \n\u001b[1;32m    733\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39m        Returns a trained MLP model.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 747\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_params()\n\u001b[1;32m    749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, incremental\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_params\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    593\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \n\u001b[1;32m    595\u001b[0m \u001b[39m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 600\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    601\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parameter_constraints,\n\u001b[1;32m    602\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    603\u001b[0m         caller_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m,\n\u001b[1;32m    604\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     constraints_str \u001b[39m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mstr\u001b[39m(c)\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mconstraints[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\u001b[39m}\u001b[39;00m\u001b[39m or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0m \u001b[39mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     98\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m{\u001b[39;00mparam_name\u001b[39m!r}\u001b[39;00m\u001b[39m parameter of \u001b[39m\u001b[39m{\u001b[39;00mcaller_name\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mconstraints_str\u001b[39m}\u001b[39;00m\u001b[39m. Got \u001b[39m\u001b[39m{\u001b[39;00mparam_val\u001b[39m!r}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'alpha' parameter of MLPClassifier must be a float in the range [0, inf). Got array([0.00321495]) instead."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function to optimize\n",
    "def objective_function(parameters):\n",
    "    learning_rate = parameters[:, 0]\n",
    "    hidden_units = np.array(parameters[:, 1], dtype=int)\n",
    "    dropout_rate = parameters[:, 2]\n",
    "    l2_reg_weight = parameters[:, 3]\n",
    "    batch_size = np.array(parameters[:, 4], dtype=int)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_units,\n",
    "                          learning_rate_init=learning_rate,\n",
    "                          alpha=l2_reg_weight,\n",
    "                          batch_size=batch_size,\n",
    "                          early_stopping=True,\n",
    "                          validation_fraction=0.1,\n",
    "                          n_iter_no_change=5,\n",
    "                          random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return -accuracy  # Negative sign for maximization\n",
    "\n",
    "# Define the bounds and types of the hyperparameters to optimize\n",
    "bounds = [{'name': 'learning_rate', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "          {'name': 'hidden_units', 'type': 'discrete', 'domain': (50, 100, 200)},\n",
    "          {'name': 'dropout_rate', 'type': 'continuous', 'domain': (0.0, 0.5)},\n",
    "          {'name': 'l2_reg_weight', 'type': 'continuous', 'domain': (0.0001, 0.01)},\n",
    "          {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 32, 64)}]\n",
    "\n",
    "# Set the number of optimization iterations\n",
    "max_iterations = 30\n",
    "\n",
    "# Set the output file for the optimization report\n",
    "report_file = 'bayes_opt.txt'\n",
    "\n",
    "# Set the checkpoint filename format\n",
    "checkpoint_format = 'checkpoint_lr_{}_hu_{}_dr_{}_l2_{}_bs_{}.pkl'\n",
    "\n",
    "# Initialize the Bayesian optimizer\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=objective_function, domain=bounds)\n",
    "\n",
    "# Run the optimization\n",
    "optimizer.run_optimization(max_iter=max_iterations, report_file=report_file, verbosity=True)\n",
    "\n",
    "# Get the best hyperparameters and objective value\n",
    "best_hyperparams = optimizer.x_opt\n",
    "best_objective = optimizer.fx_opt\n",
    "\n",
    "# Print the best hyperparameters and objective value\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(\"Learning Rate:\", best_hyperparams[0])\n",
    "print(\"Hidden Units:\", int(best_hyperparams[1]))\n",
    "print(\"Dropout Rate:\", best_hyperparams[2])\n",
    "print(\"L2 Regularization Weight:\", best_hyperparams[3])\n",
    "print(\"Batch Size:\", int(best_hyperparams[4]))\n",
    "print(\"Best Objective Value:\", -best_objective)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
