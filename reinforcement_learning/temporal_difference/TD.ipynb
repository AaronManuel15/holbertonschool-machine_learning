{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 0. Monte Carlo\"\"\"\n",
    "\n",
    "\n",
    "def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"Performs Monte Carlo evaluation for the given environment.\n",
    "    Args:\n",
    "        env: The environment to evaluate.\n",
    "        V: The value function to update.\n",
    "        policy: The policy to evaluate.\n",
    "        episodes: The number of episodes to sample.\n",
    "        max_steps: The maximum number of steps per episode.\n",
    "        alpha: The learning rate.\n",
    "        gamma: The discount factor.\n",
    "    Returns:\n",
    "        The updated value function.\n",
    "    \"\"\"\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset(seed=0)\n",
    "        episode = []\n",
    "        for _ in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            episode.append((state, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        episode = np.array(episode, dtype=int)\n",
    "        G = 0\n",
    "        for s, r in reversed(episode):\n",
    "            G = gamma * G + r\n",
    "            if s not in episode[:ep, 0]:\n",
    "                V[s] += alpha * (G - V[s])\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ediddev/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9     0.6561  0.6561  0.729   0.5905  0.81    1.      1.    ]\n",
      " [ 0.729   0.3874  0.5314  0.5314  0.5905  0.729   0.729   1.    ]\n",
      " [ 0.4305  0.3138  0.2059 -1.      1.      0.9     0.729   0.81  ]\n",
      " [ 0.81    0.4305  0.2288  0.6561  0.81   -1.      0.729   0.5314]\n",
      " [ 1.      0.5905  0.4305 -1.      1.      1.      0.9     0.81  ]\n",
      " [ 1.     -1.     -1.      1.      1.      1.     -1.      0.9   ]\n",
      " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
      " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64') \n",
    "np.set_printoptions(precision=4)\n",
    "# env.seed(0)\n",
    "print(monte_carlo(env, V, policy).reshape((8, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2: TD(位)\"\"\"\n",
    "\n",
    "\n",
    "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"Performs TD(位) evaluation for the given environment.\n",
    "    Args:\n",
    "        env: The environment to evaluate.\n",
    "        V: The value function to update.\n",
    "        policy: The policy to evaluate.\n",
    "        lambtha: The eligibility trace decay rate.\n",
    "        episodes: The number of episodes to sample.\n",
    "        max_steps: The maximum number of steps per episode.\n",
    "        alpha: The learning rate.\n",
    "        gamma: The discount factor.\n",
    "    Returns:\n",
    "        The updated value function.\n",
    "    \"\"\"\n",
    "\n",
    "    E = np.zeros((64,))\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset(seed=0)\n",
    "        for _ in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            E *= gamma * lambtha\n",
    "            E[state] += 1\n",
    "            delta = reward + gamma * V[next_state] - V[state]\n",
    "            V += alpha * delta * E\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8059 -0.8203 -0.8286 -0.8209 -0.8315 -0.7777 -0.7871 -0.6169]\n",
      " [-0.8083 -0.8615 -0.8464 -0.8357 -0.8036 -0.7733 -0.6496 -0.5982]\n",
      " [-0.8721 -0.9016 -0.951  -1.     -0.8918 -0.8751 -0.7047 -0.5271]\n",
      " [-0.9075 -0.9154 -0.951  -0.9654 -0.9606 -1.     -0.547  -0.3582]\n",
      " [-0.9112 -0.9247 -0.932  -1.     -0.8593 -0.7466 -0.6928 -0.3524]\n",
      " [-0.8534 -1.     -1.      1.     -0.147   0.1954 -1.      0.3366]\n",
      " [-0.2465 -1.      1.      1.     -1.      0.0771 -1.      1.208 ]\n",
      " [ 1.      1.      1.     -1.      1.      0.6478  1.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64') \n",
    "np.set_printoptions(precision=4)\n",
    "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2: SARSA(位)\"\"\"\n",
    "\n",
    "\n",
    "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1,\n",
    "                  gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"Perfroms SARSA(位) for the given environment.\n",
    "    Args:\n",
    "        env: The environment to evaluate.\n",
    "        Q: numpy.ndarray of shape (s,a) containing Q table.\n",
    "        lambtha: The eligibility trace decay rate.\n",
    "        episodes: The number of episodes to sample.\n",
    "        max_steps: The maximum number of steps per episode.\n",
    "        alpha: The learning rate.\n",
    "        gamma: The discount factor.\n",
    "        epsilon: The initial threshold for epsilon greedy.\n",
    "        min_epsilon: The minimum exploration rate.\n",
    "        epsilon_decay: The decay rate for exploration.\n",
    "    Returns:\n",
    "        The updated Q table.\n",
    "    \"\"\"\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        action = epsilon_greedy(Q, state, epsilon)\n",
    "        E = np.zeros((Q.shape))\n",
    "        for _ in range(max_steps):\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_action = epsilon_greedy(Q, next_state, epsilon)\n",
    "            delta = reward + gamma * Q[next_state, next_action] - Q[state, action]\n",
    "            E[state, action] += 1\n",
    "            Q += alpha * delta * E\n",
    "            E *= gamma * lambtha\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        epsilon = max(min_epsilon, epsilon * (1 - epsilon_decay))\n",
    "    return Q\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"Calculates epsilon greedy action\"\"\"\n",
    "    p = np.random.uniform()\n",
    "    if p > epsilon:\n",
    "        return np.argmax(Q[state])\n",
    "    else:\n",
    "        return np.random.randint(Q.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ediddev/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5038 0.5197 0.5231 0.5496]\n",
      " [0.5202 0.5395 0.5328 0.5705]\n",
      " [0.5045 0.5311 0.5992 0.5286]\n",
      " [0.5101 0.6131 0.4998 0.5283]\n",
      " [0.5777 0.5914 0.6389 0.5344]\n",
      " [0.6382 0.6429 0.7349 0.6463]\n",
      " [0.6589 0.6668 0.7645 0.6655]\n",
      " [0.7644 0.6905 0.652  0.6673]\n",
      " [0.5331 0.4984 0.6042 0.4867]\n",
      " [0.6848 0.5349 0.5134 0.5431]\n",
      " [0.6316 0.6052 0.5155 0.541 ]\n",
      " [0.455  0.467  0.4459 0.6472]\n",
      " [0.6435 0.5577 0.5632 0.4503]\n",
      " [0.6397 0.6155 0.7814 0.6292]\n",
      " [0.797  0.665  0.6829 0.6859]\n",
      " [0.7622 0.6463 0.6706 0.6228]\n",
      " [0.5266 0.5316 0.6804 0.5203]\n",
      " [0.5558 0.7862 0.5938 0.5646]\n",
      " [0.728  0.4675 0.5143 0.5146]\n",
      " [0.2828 0.1202 0.2961 0.1187]\n",
      " [0.4978 0.6453 0.449  0.5209]\n",
      " [0.653  0.6892 0.8251 0.7279]\n",
      " [0.7479 0.796  0.7288 0.6958]\n",
      " [0.7782 0.7459 0.6878 0.6599]\n",
      " [0.4435 0.4586 0.6811 0.5292]\n",
      " [0.5959 0.6256 0.832  0.6278]\n",
      " [0.6254 0.818  0.6004 0.6068]\n",
      " [0.6067 0.8102 0.5932 0.5548]\n",
      " [0.6609 0.6136 0.8124 0.6239]\n",
      " [0.8811 0.5813 0.8817 0.6925]\n",
      " [0.7378 0.8357 0.7684 0.7789]\n",
      " [0.7943 0.7378 0.6252 0.6248]\n",
      " [0.4851 0.4523 0.6583 0.4964]\n",
      " [0.6811 0.879  0.7068 0.7997]\n",
      " [0.7128 0.8202 0.7197 0.7246]\n",
      " [0.8965 0.3676 0.4359 0.8919]\n",
      " [0.7292 0.8627 0.2054 0.7453]\n",
      " [0.7327 0.7909 0.3653 0.8468]\n",
      " [0.4403 0.6968 0.5612 0.8298]\n",
      " [0.7998 0.8366 0.6045 0.7407]\n",
      " [0.5437 0.5703 0.4545 0.4718]\n",
      " [0.9755 0.8558 0.0117 0.36  ]\n",
      " [0.73   0.1716 0.521  0.0543]\n",
      " [0.2688 0.0185 0.8594 0.2239]\n",
      " [0.4602 0.9033 0.7206 0.1054]\n",
      " [0.2929 0.8587 0.6618 0.4714]\n",
      " [0.9342 0.614  0.5356 0.5899]\n",
      " [0.8791 0.772  0.8228 0.402 ]\n",
      " [0.4307 0.5124 0.4793 0.4323]\n",
      " [0.2274 0.2544 0.058  0.4344]\n",
      " [0.3118 0.5782 0.3778 0.2339]\n",
      " [0.0247 0.0672 0.7054 0.4537]\n",
      " [0.5366 0.8967 0.9903 0.2169]\n",
      " [0.6631 0.2633 0.1028 0.8197]\n",
      " [0.32   0.3835 0.5883 0.831 ]\n",
      " [0.6639 0.8861 0.4708 0.8692]\n",
      " [0.3056 0.4616 0.5665 0.3366]\n",
      " [0.468  0.5731 0.3223 0.2133]\n",
      " [0.6132 0.0257 0.2739 0.4247]\n",
      " [0.3742 0.4636 0.2776 0.5868]\n",
      " [0.8639 0.1175 0.5174 0.1321]\n",
      " [0.7169 0.3961 0.5654 0.1833]\n",
      " [0.1448 0.4881 0.3556 0.9404]\n",
      " [0.7653 0.7487 0.9037 0.0834]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "Q = np.random.uniform(size=(64, 4))\n",
    "np.set_printoptions(precision=4)\n",
    "print(sarsa_lambtha(env, Q, 0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
