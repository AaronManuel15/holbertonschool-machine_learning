{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Load the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 0. Load the Environment\"\"\"\n",
    "import gym\n",
    "\n",
    "\n",
    "def load_frozen_lake(desc=None, map_name=None, is_slippery=False):\n",
    "    \"\"\"Load the FrozenLakeEnv environment from OpenAI Gym\"\"\"\n",
    "    env = gym.make(\"FrozenLake-v1\",\n",
    "                   desc=desc,\n",
    "                   map_name=map_name,\n",
    "                   is_slippery=is_slippery,\n",
    "                   render_mode=\"ansi\")\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F' b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'F' b'F' b'H' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'G']]\n",
      "[(1.0, 0, 0.0, False)]\n",
      "[[b'S' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'H' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'G']]\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 8, 0.0, True)]\n",
      "[[b'S' b'F' b'F']\n",
      " [b'F' b'H' b'H']\n",
      " [b'F' b'F' b'G']]\n",
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "env = load_frozen_lake()\n",
    "print(env.desc)\n",
    "print(env.P[0][0])\n",
    "env = load_frozen_lake(is_slippery=True)\n",
    "print(env.desc)\n",
    "print(env.P[0][0])\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "print(env.desc)\n",
    "env = load_frozen_lake(map_name='4x4')\n",
    "print(env.desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1. Initialize the Q-table\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def q_init(env):\n",
    "    \"\"\"Initialize the Q-table\"\"\"\n",
    "    action_space_size = env.action_space.n\n",
    "    state_space_size = env.observation_space.n\n",
    "    q_table = np.zeros((state_space_size, action_space_size))\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "(64, 4)\n",
      "(9, 4)\n",
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "env = load_frozen_lake()\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "env = load_frozen_lake(is_slippery=True)\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "env = load_frozen_lake(map_name='4x4')\n",
    "Q = q_init(env)\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2. Epsilon Greedy\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"Uses Epsilon Greedy to Determine the next action.\n",
    "    Args:\n",
    "        Q: numpy.ndarray containing the q-table\n",
    "        state: Current state\n",
    "        epsilon: epsilon to use for the calculation\n",
    "    Returns:\n",
    "        the next action index\"\"\"\n",
    "    \n",
    "    p = np.random.uniform()\n",
    "    if p < epsilon:\n",
    "        return np.random.randint(4)\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "Q[7] = np.array([0.5, 0.7, 1, -1])\n",
    "np.random.seed(0)\n",
    "print(epsilon_greedy(Q, 7, 0.5))\n",
    "np.random.seed(1)\n",
    "print(epsilon_greedy(Q, 7, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 3. Q-learning\"\"\"\n",
    "\n",
    "\n",
    "def train(env, Q, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99,\n",
    "          epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"Performs Q-learning:\n",
    "    Args:\n",
    "        env: the FrozenLakeEnv instance\n",
    "        Q: numpy.ndarray containging the Q-table\n",
    "        episodes: total number of episodes to train over\n",
    "        max_steps: maximum number of steps per episode\n",
    "        alpha: learning rate\n",
    "        gamma: discount rate\n",
    "        epsilon: initial threshold for epsilon greedy\n",
    "        min_epsilon: minimum value for updating epsiolon between episodes\n",
    "    Return:\n",
    "        Q: the updated Q-table\n",
    "        total_rewards: list containing the rewards per episode\"\"\"\n",
    "\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            action = epsilon_greedy(Q, state, epsilon)\n",
    "            new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            if reward == 1 and done:\n",
    "                rewards_current_episode += reward\n",
    "            elif reward == 0 and done:\n",
    "                rewards_current_episode -= 1\n",
    "                reward = -1\n",
    "\n",
    "            Q[state, action] = (1 - alpha) * Q[state, action] + \\\n",
    "                alpha * (reward + gamma * np.max(Q[new_state]))\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        epsilon = (1 - min_epsilon) * np.exp(-epsilon_decay * episode) +\\\n",
    "            min_epsilon\n",
    "        total_rewards.append(rewards_current_episode)\n",
    "\n",
    "    return Q, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.96059593  0.970299    0.95098488  0.96059396]\n",
      " [ 0.96059557 -0.77123208  0.0094072   0.37627228]\n",
      " [ 0.18061285 -0.1         0.          0.        ]\n",
      " [ 0.97029877  0.9801     -0.99999988  0.96059583]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.98009763  0.98009933  0.99        0.9702983 ]\n",
      " [ 0.98009922  0.98999782  1.         -0.99999952]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "500 : 0.812\n",
      "1000 : 0.88\n",
      "1500 : 0.9\n",
      "2000 : 0.9\n",
      "2500 : 0.88\n",
      "3000 : 0.844\n",
      "3500 : 0.892\n",
      "4000 : 0.896\n",
      "4500 : 0.852\n",
      "5000 : 0.928\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "\n",
    "Q, total_rewards  = train(env, Q)\n",
    "print(Q)\n",
    "split_rewards = np.split(np.array(total_rewards), 10)\n",
    "for i, rewards in enumerate(split_rewards):\n",
    "    print((i+1) * 500, ':', np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, Q, max_steps=100):\n",
    "    \"\"\"Plays an episode using the trained Q-table\n",
    "    Args:\n",
    "        env: FrozenLakeEnv instance\n",
    "        Q: numpy.ndarray shape (state, action) containing the trained Q-table\n",
    "        max_steps: Maximum number of steps in the episode\n",
    "    Returns:\n",
    "        total_rewards: the total rewards for the episode\n",
    "    \"\"\"\n",
    "    total_rewards = 0\n",
    "    state, _ = env.reset()\n",
    "    print(env.render())\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = np.argmax(Q[state])\n",
    "        state, reward, terminated, _, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "        print(env.render())\n",
    "        if terminated:\n",
    "            break\n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFF\n",
      "FHH\n",
      "FFG\n",
      "\n",
      "  (Down)\n",
      "SFF\n",
      "\u001b[41mF\u001b[0mHH\n",
      "FFG\n",
      "\n",
      "  (Down)\n",
      "SFF\n",
      "FHH\n",
      "\u001b[41mF\u001b[0mFG\n",
      "\n",
      "  (Right)\n",
      "SFF\n",
      "FHH\n",
      "F\u001b[41mF\u001b[0mG\n",
      "\n",
      "  (Right)\n",
      "SFF\n",
      "FHH\n",
      "FF\u001b[41mG\u001b[0m\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "\n",
    "Q, total_rewards  = train(env, Q)\n",
    "print(play(env, Q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
