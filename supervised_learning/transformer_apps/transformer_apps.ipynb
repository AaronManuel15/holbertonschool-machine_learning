{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 11:35:54.245285: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-17 11:35:54.558024: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-17 11:35:55.325933: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-08-17 11:35:55.326107: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-08-17 11:35:55.326115: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/ediddev/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 10:59:05.248578: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ediddev/.local/lib/python3.8/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:\n",
      "2023-08-17 10:59:05.248651: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-08-17 10:59:05.248680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (BOBO-CODE): /proc/driver/nvidia/version does not exist\n",
      "2023-08-17 10:59:05.249238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 10:59:05.576452: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "pt2en_train = tfds.load('ted_hrlr_translate/pt_to_en', split='train', as_supervised=True)\n",
    "for pt, en in pt2en_train.take(1):\n",
    "  print(pt.numpy().decode('utf-8'))\n",
    "  print(en.numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 0. Dataset\"\"\"\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"loads and preps a dataset for machine translation\"\"\"\n",
    "    def __init__(self, batch_size, max_len) -> None:\n",
    "        self.data_train = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "                                    split='train',\n",
    "                                    as_supervised=True)\n",
    "        self.data_valid = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "                                    split='validation',\n",
    "                                    as_supervised=True)\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(self.data_train)\n",
    "\n",
    "        self.data_train = self.data_train.map(self.tf_encode)\n",
    "        self.data_valid = self.data_valid.map(self.tf_encode)\n",
    "\n",
    "        def filter_max_length(x, y, max_length=self.max_len):\n",
    "            \"\"\"filter method\"\"\"\n",
    "            return tf.logical_and(tf.size(x) <= max_length,\n",
    "                                  tf.size(y) <= max_length)\n",
    "        self.data_train = self.data_train.filter(filter_max_length)\n",
    "        self.data_train = self.data_train.cache()\n",
    "        self.data_train = self.data_train.shuffle(2**15, reshuffle_each_iteration=True).padded_batch(self.batch_size)\n",
    "        self.data_train = self.data_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        self.data_valid = self.data_valid.filter(filter_max_length).padded_batch(self.batch_size)\n",
    "\n",
    "    def tokenize_dataset(self, data):\n",
    "        \"\"\"creates sub-word tokenizers for our dataset\"\"\"\n",
    "        Subword = tfds.deprecated.text.SubwordTextEncoder\n",
    "        tokenizer_pt = Subword.build_from_corpus((pt.numpy() for pt, _ in data),\n",
    "                                                 target_vocab_size=2**15)\n",
    "        tokenizer_en = Subword.build_from_corpus((en.numpy() for _, en in data),\n",
    "                                                 target_vocab_size=2**15)\n",
    "        return tokenizer_pt, tokenizer_en\n",
    "\n",
    "    def encode(self, pt, en):\n",
    "        \"\"\"encodes a translation into tokens\"\"\"\n",
    "        pt_vsize = self.tokenizer_pt.vocab_size\n",
    "        en_vsize = self.tokenizer_en.vocab_size\n",
    "        pt_tokens = [pt_vsize] + self.tokenizer_pt.encode(pt.numpy()) + \\\n",
    "                    [pt_vsize + 1]\n",
    "        en_tokens = [en_vsize] + self.tokenizer_en.encode(en.numpy()) + \\\n",
    "                    [en_vsize + 1]\n",
    "        return pt_tokens, en_tokens\n",
    "\n",
    "    def tf_encode(self, pt, en):\n",
    "        \"\"\"acts as a tensorflow wrapper for the encode instance method\"\"\"\n",
    "        pt_lang, en_lang = tf.py_function(func=self.encode,\n",
    "                                          inp=[pt, en],\n",
    "                                          Tout=[tf.int64, tf.int64])\n",
    "        pt_lang.set_shape([None])\n",
    "        en_lang.set_shape([None])\n",
    "        return pt_lang, en_lang\n",
    "\n",
    "    def create_masks(self, batch):\n",
    "        \"\"\"creates all masks for training/validation\"\"\"\n",
    "        def create_padding_mask(seq):\n",
    "            \"\"\"creates padding mask\"\"\"\n",
    "            mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "            return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "        def create_look_ahead_mask(size):\n",
    "            \"\"\"creates look ahead mask\"\"\"\n",
    "            mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "            return mask\n",
    "\n",
    "        def create_masks_decoder(tar):\n",
    "            \"\"\"creates all masks for training/validation\"\"\"\n",
    "            look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "            dec_target_padding_mask = create_padding_mask(tar)\n",
    "            combined_mask = tf.maximum(dec_target_padding_mask,\n",
    "                                       look_ahead_mask)\n",
    "            return combined_mask\n",
    "\n",
    "        enc_padding_mask = create_padding_mask(batch[0])\n",
    "        dec_padding_mask = create_padding_mask(batch[0])\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(batch[1])[1])\n",
    "        dec_target_padding_mask = create_padding_mask(batch[1])\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask,\n",
    "                                   look_ahead_mask)\n",
    "        return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 15:16:11.733229: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 12246 of 32768\n",
      "2023-08-17 15:16:21.733139: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 28146 of 32768\n",
      "2023-08-17 15:16:24.368854: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2023-08-17 15:16:24.390258: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_masks() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[39m=\u001b[39m Dataset(\u001b[39m32\u001b[39m, \u001b[39m40\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m inputs, target \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mdata_train\u001b[39m.\u001b[39mtake(\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mprint\u001b[39m(data\u001b[39m.\u001b[39;49mcreate_masks(inputs, target))\n",
      "\u001b[0;31mTypeError\u001b[0m: create_masks() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.set_random_seed(0)\n",
    "data = Dataset(32, 40)\n",
    "for inputs, target in data.data_train.take(1):\n",
    "    print(data.create_masks(inputs, target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
