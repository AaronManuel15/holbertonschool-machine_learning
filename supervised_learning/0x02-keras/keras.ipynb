{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow.compat.v1.keras as K\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 0\"\"\"\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"Builds a neural network with Keras\n",
    "    nx: is the number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer\n",
    "        of the network\n",
    "    activations: list containing the activation functions\n",
    "        used for each layer of the network\n",
    "    lambtha: is the L2 regularization parameter\n",
    "    keep_prob: is the probability that a node will be kept for dropout\n",
    "\n",
    "    Returns: the keras model\n",
    "    \"\"\"\n",
    "    reg = K.regularizers.l2(lambtha)\n",
    "    model = K.Sequential()\n",
    "    for i in range(len(layers)):\n",
    "        if i == 0:\n",
    "            model.add(K.layers.Dense(layers[i], activation=activations[i],\n",
    "                                     kernel_regularizer=reg,\n",
    "                                     input_shape=(nx,)))\n",
    "        else:\n",
    "            model.add(K.layers.Dropout(1 - keep_prob))\n",
    "            model.add(K.layers.Dense(layers[i], activation=activations[i],\n",
    "                                     kernel_regularizer=reg))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 0: main file\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1\"\"\"\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"Builds a neural network with Keras\n",
    "    nx: is the number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer\n",
    "        of the network\n",
    "    activations: list containing the activation functions\n",
    "        used for each layer of the network\n",
    "    lambtha: is the L2 regularization parameter\n",
    "    keep_prob: is the probability that a node will be kept for dropout\n",
    "\n",
    "    Returns: the keras model\"\"\"\n",
    "    reg = K.regularizers.l2(lambtha)\n",
    "\n",
    "    inputs = K.Input(shape=(nx,))\n",
    "    for i in range(len(layers)):\n",
    "        if i == 0:\n",
    "            layer = K.layers.Dense(layers[i], activation=activations[i],\n",
    "                                   kernel_regularizer=reg)(inputs)\n",
    "        else:\n",
    "            layer = K.layers.Dense(layers[i], activation=activations[i],\n",
    "                                   kernel_regularizer=reg)(layer)\n",
    "        if i < len(layers) - 1:\n",
    "            layer = K.layers.Dropout(1 - keep_prob)(layer)\n",
    "    model = K.Model(inputs=inputs, outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 1: main file\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2\"\"\"\n",
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\"Sets up Adam optimization for a keras model with categorical\n",
    "        crossentropy loss and accuracy metrics.\n",
    "        Args:\n",
    "            network: is the model to optimize\n",
    "            alpha: is the learning rate\n",
    "            beta1: is the first Adam optimization parameter\n",
    "            beta2: is the second Adam optimization parameter\n",
    "\n",
    "        Returns: \n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    network.compile(optimizer=K.optimizers.Adam(lr=alpha,\n",
    "                                                beta_1=beta1,\n",
    "                                                beta_2=beta2),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 2: main file\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    optimize_model(model, 0.01, 0.99, 0.9)\n",
    "    print(model.loss)\n",
    "    print(model.metrics)\n",
    "    opt = model.optimizer\n",
    "    print(opt.__class__)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #print(sess.run([opt.lr, opt.beta_1, opt.beta_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 3\"\"\"\n",
    "def one_hot(labels, classes=None):\n",
    "    \"\"\"Converts a label vector into a one-hot matrix\n",
    "    Args:\n",
    "        labels: is a numpy.ndarray with shape (m,) containing numeric class labels\n",
    "        classes: is the maximum number of classes found in labels\n",
    "    Conditions:\n",
    "        The last dimension of the one-hot matrix must be the number of classes\n",
    "    Returns:\n",
    "        one-hot matrix\"\"\"\n",
    "\n",
    "    return K.utils.to_categorical(labels, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Task 3: main file\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "    print(labels)\n",
    "    print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 4\"\"\"\n",
    "def train_model(network, data, labels, batch_size, epochs, verbose=True, shuffle=False):\n",
    "    \"\"\"Trains a model using mini-batch gradient descent\n",
    "    Args:\n",
    "        network: is the model to train\n",
    "        data: is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "        labels: is a one-hot numpy.ndarray of shape (m, classes) containing the labels of data\n",
    "        batch_size: is the size of the batch used for mini-batch gradient descent\n",
    "        epochs: is the number of passes through data for mini-batch gradient descent\n",
    "        verbose: is a boolean that determines if output should be printed during training\n",
    "        shuffle: is a boolean that determines whether to shuffle the batches every epoch.\n",
    "            Normally, it is a good idea to shuffle, but for reproducibility, we have chosen\n",
    "            to set the default to False.\n",
    "    Returns: the History object generated after training the model\"\"\"\n",
    "\n",
    "    return network.fit(data, labels, batch_size=batch_size, epochs=epochs,\n",
    "                       verbose=verbose, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_472/4100387864.py:14: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ediddev/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 15:56:08.601623: W tensorflow/c/c_api.cc:291] Operation '{name:'decay_10/Assign' id:3036 op device:{requested: '', assigned: ''} def:{{{node decay_10/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](decay_10, decay_10/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.3291 - accuracy: 0.9216\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 4s 80us/sample - loss: 0.1764 - accuracy: 0.9651\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 5s 93us/sample - loss: 0.1414 - accuracy: 0.9757\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 4s 82us/sample - loss: 0.1280 - accuracy: 0.9797\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 3s 66us/sample - loss: 0.1146 - accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Task 4: main file\"\"\"\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.set_random_seed(SEED)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Task 5\"\"\"\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\"Trains a model using mini-batch gradient descent and also validates the model\n",
    "    Args:\n",
    "        network: is the model to train\n",
    "        data: is a numpy.ndarray of shape (m, nx) containing the input data\n",
    "        labels: is a one-hot numpy.ndarray of shape (m, classes) containing the labels of data\n",
    "        batch_size: is the size of the batch used for mini-batch gradient descent\n",
    "        epochs: is the number of passes through data for mini-batch gradient descent\n",
    "        validation_data: is the data to validate the model with, if not None\n",
    "        verbose: is a boolean that determines if output should be printed during training\n",
    "        shuffle: is a boolean that determines whether to shuffle the batches every epoch.\n",
    "            Normally, it is a good idea to shuffle, but for reproducibility, we have chosen\n",
    "            to set the default to False.\n",
    "    Returns: the History object generated after training the model\"\"\"\n",
    "\n",
    "    return network.fit(data, labels, batch_size=batch_size,\n",
    "                       epochs=epochs,validation_data=validation_data,\n",
    "                       verbose=verbose, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ediddev/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-04 16:11:46.168634: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/dense_57/bias/m/Assign' id:4430 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_57/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_57/bias/m, training_2/Adam/dense_57/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49600/50000 [============================>.] - ETA: 0s - loss: 0.3297 - accuracy: 0.9214"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ediddev/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-03-04 16:11:50.743997: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_16/AddN_1' id:4305 op device:{requested: '', assigned: ''} def:{{{node loss_16/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul, loss_16/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 97us/sample - loss: 0.3291 - accuracy: 0.9216 - val_loss: 0.1841 - val_accuracy: 0.9642\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 4s 83us/sample - loss: 0.1764 - accuracy: 0.9651 - val_loss: 0.1545 - val_accuracy: 0.9719\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 4s 89us/sample - loss: 0.1414 - accuracy: 0.9757 - val_loss: 0.1417 - val_accuracy: 0.9746\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 5s 90us/sample - loss: 0.1280 - accuracy: 0.9797 - val_loss: 0.1422 - val_accuracy: 0.9766\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 4s 74us/sample - loss: 0.1146 - accuracy: 0.9838 - val_loss: 0.1398 - val_accuracy: 0.9769\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Task 5: main file\"\"\"\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.set_random_seed(SEED)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
